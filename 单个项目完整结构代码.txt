é¡¹ç›® 'decopy-2api' çš„ç»“æ„æ ‘:
ğŸ“‚ decopy-2api/
    ğŸ“„ .env
    ğŸ“„ .env.example
    ğŸ“„ Dockerfile
    ğŸ“„ docker-compose.yml
    ğŸ“„ main.py
    ğŸ“„ nginx.conf
    ğŸ“„ requirements.txt
    ğŸ“‚ app/
        ğŸ“‚ core/
            ğŸ“„ __init__.py
            ğŸ“„ config.py
        ğŸ“‚ providers/
            ğŸ“„ __init__.py
            ğŸ“„ base_provider.py
            ğŸ“„ decopy_provider.py
        ğŸ“‚ utils/
            ğŸ“„ sse_utils.py
================================================================================

--- æ–‡ä»¶è·¯å¾„: .env ---

# ====================================================================
# decopy-2api é…ç½®æ–‡ä»¶æ¨¡æ¿
# ====================================================================
#
# è¯·å°†æ­¤æ–‡ä»¶é‡å‘½åä¸º ".env" å¹¶æŒ‰éœ€ä¿®æ”¹ã€‚
#

# --- æ ¸å¿ƒå®‰å…¨é…ç½® (å¿…é¡»è®¾ç½®) ---
# ç”¨äºä¿æŠ¤æ‚¨ API æœåŠ¡çš„è®¿é—®å¯†é’¥ã€‚
API_MASTER_KEY=1

# --- éƒ¨ç½²é…ç½® (å¯é€‰) ---
# Nginx å¯¹å¤–æš´éœ²çš„ç«¯å£
NGINX_PORT=8088

# --- Decopy.ai é™æ€å‡­è¯ (å¿…é¡»è®¾ç½®) ---
# å·²ä»æ‚¨çš„æŠ“åŒ…æ•°æ®ä¸­è‡ªåŠ¨æå–ã€‚é€šå¸¸æ— éœ€ä¿®æ”¹ã€‚
PRODUCT_CODE="067003"
PRODUCT_SERIAL="eb0f5222701cbd6e67799c0cb99ec32b"


--- æ–‡ä»¶è·¯å¾„: .env.example ---

# ====================================================================
# decopy-2api é…ç½®æ–‡ä»¶æ¨¡æ¿
# ====================================================================
#
# è¯·å°†æ­¤æ–‡ä»¶é‡å‘½åä¸º ".env" å¹¶æŒ‰éœ€ä¿®æ”¹ã€‚
#

# --- æ ¸å¿ƒå®‰å…¨é…ç½® (å¿…é¡»è®¾ç½®) ---
# ç”¨äºä¿æŠ¤æ‚¨ API æœåŠ¡çš„è®¿é—®å¯†é’¥ã€‚
API_MASTER_KEY=sk-decopy-2api-default-key-please-change-me

# --- éƒ¨ç½²é…ç½® (å¯é€‰) ---
# Nginx å¯¹å¤–æš´éœ²çš„ç«¯å£
NGINX_PORT=8088

# --- Decopy.ai é™æ€å‡­è¯ (å¿…é¡»è®¾ç½®) ---
# å·²ä»æ‚¨çš„æŠ“åŒ…æ•°æ®ä¸­è‡ªåŠ¨æå–ã€‚é€šå¸¸æ— éœ€ä¿®æ”¹ã€‚
PRODUCT_CODE="067003"
PRODUCT_SERIAL="eb0f5222701cbd6e67799c0cb99ec32b"


--- æ–‡ä»¶è·¯å¾„: Dockerfile ---

# ====================================================================
# Dockerfile for decopy-2api (v1.0.2 - Hotfix)
# ====================================================================

FROM python:3.10-slim

ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1
WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r requirements.txt

COPY . .

# ã€ä¿®æ­£ã€‘å°†ç”¨æˆ·åˆ›å»ºæå‰ï¼Œå¹¶ç§»é™¤ä¸å†éœ€è¦çš„æ—¥å¿—ç›®å½•æ“ä½œ
RUN useradd --create-home appuser && \
    chown -R appuser:appuser /app
USER appuser

EXPOSE 8000
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "1"]


--- æ–‡ä»¶è·¯å¾„: docker-compose.yml ---

services:
  nginx:
    image: nginx:latest
    container_name: decopy-2api-nginx
    restart: always
    ports:
      - "${NGINX_PORT:-8088}:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - app
    networks:
      - decopy-net

  app:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: decopy-2api-app
    restart: unless-stopped
    env_file:
      - .env
    # ã€ä¿®æ­£ã€‘ç§»é™¤ä¸å†éœ€è¦çš„ volumes æŒ‚è½½
    networks:
      - decopy-net

networks:
  decopy-net:
    driver: bridge


--- æ–‡ä»¶è·¯å¾„: main.py ---

import sys
import json
import uuid
import time
from contextlib import asynccontextmanager
from typing import Optional

from fastapi import FastAPI, Request, HTTPException, Depends, Header
from fastapi.responses import JSONResponse, StreamingResponse
from loguru import logger

from app.core.config import settings
from app.providers.decopy_provider import DecopyProvider

# --- é…ç½® Loguru ---
logger.remove()
# ã€ä¿®æ­£ã€‘å°†ä¸»æ—¥å¿—çº§åˆ«è®¾ç½®ä¸º DEBUGï¼Œä»¥åœ¨æ§åˆ¶å°æ˜¾ç¤ºæ‰€æœ‰è¯¦ç»†æ—¥å¿—
logger.add(
    sys.stdout,
    level="DEBUG", 
    format="<green>{time:YYYY-MM-DD HH:mm:ss.SSS}</green> | "
           "<level>{level: <8}</level> | "
           "<cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>",
    colorize=True,
    serialize=False
)

# --- å…¨å±€ Provider å®ä¾‹ ---
provider = DecopyProvider()

@asynccontextmanager
async def lifespan(app: FastAPI):
    logger.info(f"åº”ç”¨å¯åŠ¨ä¸­... {settings.APP_NAME} v{settings.APP_VERSION}")
    logger.info("æœåŠ¡å·²è¿›å…¥ 'Cloudscraper & Async-Job-Polling & Pseudo-Stream' æ¨¡å¼ã€‚")
    logger.info(f"æœåŠ¡å°†åœ¨ http://localhost:{settings.NGINX_PORT} ä¸Šå¯ç”¨")
    yield
    logger.info("åº”ç”¨å…³é—­ã€‚")

app = FastAPI(
    title=settings.APP_NAME,
    version=settings.APP_VERSION,
    description=settings.DESCRIPTION,
    lifespan=lifespan
)

# --- å®‰å…¨ä¾èµ– ---
async def verify_api_key(authorization: Optional[str] = Header(None)):
    if settings.API_MASTER_KEY and settings.API_MASTER_KEY != "1":
        if not authorization or "bearer" not in authorization.lower():
            raise HTTPException(status_code=401, detail="éœ€è¦ Bearer Token è®¤è¯ã€‚")
        token = authorization.split(" ")[-1]
        if token != settings.API_MASTER_KEY:
            raise HTTPException(status_code=403, detail="æ— æ•ˆçš„ API Keyã€‚")

# --- API è·¯ç”± ---
@app.post("/v1/chat/completions", dependencies=[Depends(verify_api_key)])
async def chat_completions(request: Request) -> StreamingResponse:
    try:
        request_data = await request.json()
        logger.info(f"æ”¶åˆ°å®¢æˆ·ç«¯è¯·æ±‚ /v1/chat/completions:\n{json.dumps(request_data, indent=2, ensure_ascii=False)}")
        return await provider.chat_completion(request_data)
    except Exception as e:
        logger.error(f"å¤„ç†èŠå¤©è¯·æ±‚æ—¶å‘ç”Ÿé¡¶å±‚é”™è¯¯: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"å†…éƒ¨æœåŠ¡å™¨é”™è¯¯: {str(e)}")

@app.get("/v1/models", dependencies=[Depends(verify_api_key)], response_class=JSONResponse)
async def list_models():
    return await provider.get_models()

@app.get("/", summary="æ ¹è·¯å¾„", include_in_schema=False)
def root():
    return {"message": f"æ¬¢è¿æ¥åˆ° {settings.APP_NAME} v{settings.APP_VERSION}. æœåŠ¡è¿è¡Œæ­£å¸¸ã€‚"}


--- æ–‡ä»¶è·¯å¾„: nginx.conf ---

worker_processes auto;

events {
    worker_connections 1024;
}

http {
    upstream decopy_backend {
        server app:8000;
    }

    server {
        listen 80;
        server_name localhost;

        client_max_body_size 50M; # å…è®¸ä¸Šä¼ å›¾ç‰‡

        location / {
            proxy_pass http://decopy_backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            proxy_buffering off;
            proxy_cache off;
            proxy_set_header Connection '';
            proxy_http_version 1.1;
            chunked_transfer_encoding off;
        }
    }
}


--- æ–‡ä»¶è·¯å¾„: requirements.txt ---

fastapi
uvicorn[standard]
pydantic-settings
python-dotenv
cloudscraper
httpx
loguru


--- æ–‡ä»¶è·¯å¾„: app\core\__init__.py ---



--- æ–‡ä»¶è·¯å¾„: app\core\config.py ---

from pydantic_settings import BaseSettings, SettingsConfigDict
from typing import List, Optional, Dict

class Settings(BaseSettings):
    model_config = SettingsConfigDict(
        env_file=".env",
        env_file_encoding='utf-8',
        extra="ignore"
    )

    APP_NAME: str = "decopy-2api"
    APP_VERSION: str = "1.0.0"
    DESCRIPTION: str = "ä¸€ä¸ªå°† decopy.ai è½¬æ¢ä¸ºå…¼å®¹ OpenAI æ ¼å¼ API çš„é«˜æ€§èƒ½ä»£ç†ã€‚"

    API_MASTER_KEY: Optional[str] = None
    
    # Decopy.ai é™æ€å‡­è¯
    PRODUCT_CODE: Optional[str] = "067003"
    PRODUCT_SERIAL: Optional[str] = "eb0f5222701cbd6e67799c0cb99ec32b"

    API_REQUEST_TIMEOUT: int = 180
    NGINX_PORT: int = 8088

    # æ¨¡å‹åç§°æ˜ å°„
    MODEL_MAPPING: Dict[str, str] = {
        "decopy-deepseek-v3": "DeepSeek-V3",
        "decopy-gemini-2.0-flash": "Gemini-2.0-Flash",
        "decopy-gpt-4o-mini": "GPT-4o-mini",
        "decopy-deepseek-r1": "DeepSeek-R1"
    }
    DEFAULT_MODEL: str = "decopy-deepseek-v3"

settings = Settings()


--- æ–‡ä»¶è·¯å¾„: app\providers\__init__.py ---



--- æ–‡ä»¶è·¯å¾„: app\providers\base_provider.py ---

from abc import ABC, abstractmethod
from typing import Dict, Any
from fastapi.responses import StreamingResponse, JSONResponse

class BaseProvider(ABC):
    @abstractmethod
    async def chat_completion(
        self,
        request_data: Dict[str, Any]
    ) -> StreamingResponse:
        pass

    @abstractmethod
    async def get_models(self) -> JSONResponse:
        pass


--- æ–‡ä»¶è·¯å¾„: app\providers\decopy_provider.py ---

import time
import json
import uuid
import asyncio
import base64
from typing import Dict, Any, AsyncGenerator, List

import cloudscraper
import httpx
from fastapi import HTTPException
from fastapi.responses import StreamingResponse, JSONResponse
from loguru import logger

from app.core.config import settings
from app.providers.base_provider import BaseProvider
from app.utils.sse_utils import create_sse_data, create_chat_completion_chunk, DONE_CHUNK

class DecopyProvider(BaseProvider):
    def __init__(self):
        self.scraper = cloudscraper.create_scraper()
        self.create_job_url = "https://api.decopy.ai/api/decopy/ask-ai/create-job"
        self.get_job_url_template = "https://api.decopy.ai/api/decopy/ask-ai/get-job/{job_id}"

    async def chat_completion(self, request_data: Dict[str, Any]) -> StreamingResponse:
        
        async def stream_generator() -> AsyncGenerator[bytes, None]:
            request_id = f"chatcmpl-{uuid.uuid4()}"
            model_name = request_data.get("model", settings.DEFAULT_MODEL)
            
            try:
                headers = self._prepare_headers()
                files, chat_id = await self._prepare_form_data(request_data)
                
                logger.info(f"å‘ Decopy æäº¤ä»»åŠ¡, æ¨¡å‹: {model_name}, Chat ID: {chat_id}")

                logger.debug(f"--- [REQUEST TO DECOPY (CREATE JOB)] ---\nURL: POST {self.create_job_url}\nHeaders: {headers}\nPayload (multipart): Omitted for brevity\n-----------------------------------------")

                loop = asyncio.get_running_loop()
                response = await loop.run_in_executor(
                    None, 
                    lambda: self.scraper.post(self.create_job_url, headers=headers, files=files)
                )
                
                response_json = response.json()
                logger.debug(f"--- [RESPONSE FROM DECOPY (CREATE JOB)] ---\nStatus: {response.status_code}\nBody: {json.dumps(response_json, indent=2, ensure_ascii=False)}\n-------------------------------------------")

                response.raise_for_status()
                
                if response_json.get("code") != 100000:
                    raise HTTPException(status_code=502, detail=f"ä¸Šæ¸¸åˆ›å»ºä»»åŠ¡å¤±è´¥: {response_json.get('message', {}).get('zh', 'æœªçŸ¥é”™è¯¯')}")
                
                job_id = response_json.get("result", {}).get("job_id")
                if not job_id:
                    raise HTTPException(status_code=502, detail="ä¸Šæ¸¸å“åº”ä¸­æœªæ‰¾åˆ° job_idã€‚")
                
                logger.info(f"ä»»åŠ¡æäº¤æˆåŠŸ, Job ID: {job_id}. å¼€å§‹è·å– SSE æµ...")

                stream_url = self.get_job_url_template.format(job_id=job_id)
                
                logger.debug(f"--- [REQUEST TO DECOPY (GET STREAM)] ---\nURL: GET {stream_url}\nHeaders: {headers}\n----------------------------------------")
                
                stream_response = await loop.run_in_executor(
                    None,
                    lambda: self.scraper.get(stream_url, headers=headers, stream=True)
                )
                
                logger.info(f"è·å– SSE æµå“åº”çŠ¶æ€ç : {stream_response.status_code}")
                stream_response.raise_for_status()

                # ã€æœ€ç»ˆä¿®æ­£ã€‘æ­£ç¡®å¤„ç†å¢é‡JSONæµ
                for line in stream_response.iter_lines():
                    logger.trace(f"Raw SSE line: {line}")
                    if line.startswith(b"data:"):
                        content_str = line[len(b"data:"):].strip().decode('utf-8', errors='ignore')
                        
                        if content_str == 'Data transfer completed.':
                            logger.info("æ£€æµ‹åˆ° 'Data transfer completed.' æ¶ˆæ¯ï¼Œæµç»“æŸã€‚")
                            continue
                        
                        if not content_str:
                            continue
                            
                        try:
                            # è§£ææ¯ä¸€è¡Œç‹¬ç«‹çš„ JSON å¯¹è±¡
                            data = json.loads(content_str)
                            # æå– 'data' å­—æ®µä½œä¸ºå¢é‡å†…å®¹
                            delta_content = data.get("data")
                            
                            if delta_content is not None:
                                # ç›´æ¥å°†å¢é‡å†…å®¹å°è£…å¹¶å‘é€
                                chunk = create_chat_completion_chunk(request_id, model_name, delta_content)
                                yield create_sse_data(chunk)
                        except json.JSONDecodeError:
                            logger.warning(f"æ— æ³•è§£æ SSE æ•°æ®å—: {content_str}")
                            continue
                
                logger.info(f"Job ID: {job_id} çš„æµå¤„ç†å®Œæ¯•ã€‚")
                final_chunk = create_chat_completion_chunk(request_id, model_name, "", "stop")
                yield create_sse_data(final_chunk)
                yield DONE_CHUNK

            except Exception as e:
                logger.error(f"å¤„ç†æµæ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
                error_message = f"å†…éƒ¨æœåŠ¡å™¨é”™è¯¯: {str(e)}"
                error_chunk = create_chat_completion_chunk(request_id, model_name, error_message, "stop")
                yield create_sse_data(error_chunk)
                yield DONE_CHUNK

        return StreamingResponse(stream_generator(), media_type="text/event-stream")

    def _prepare_headers(self) -> Dict[str, str]:
        return {
            "Accept": "*/*",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Origin": "https://decopy.ai",
            "Referer": "https://decopy.ai/",
            "product-code": settings.PRODUCT_CODE,
            "product-serial": settings.PRODUCT_SERIAL,
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36",
        }

    async def _prepare_form_data(self, request_data: Dict[str, Any]) -> tuple[Dict[str, Any], str]:
        messages = request_data.get("messages", [])
        if not messages:
            raise HTTPException(status_code=400, detail="è¯·æ±‚ä¸­ç¼ºå°‘ 'messages' å­—æ®µã€‚")

        last_user_message = next((m for m in reversed(messages) if m.get("role") == "user"), None)
        if not last_user_message or not last_user_message.get("content"):
            raise HTTPException(status_code=400, detail="æœªæ‰¾åˆ°æœ‰æ•ˆçš„ç”¨æˆ·æ¶ˆæ¯ã€‚")

        prompt_text = ""
        image_url = None
        
        content = last_user_message["content"]
        if isinstance(content, str):
            prompt_text = content
        elif isinstance(content, list):
            for part in content:
                if part.get("type") == "text":
                    prompt_text = part.get("text", "")
                elif part.get("type") == "image_url":
                    image_url = part.get("image_url", {}).get("url")

        model_name = request_data.get("model", settings.DEFAULT_MODEL)
        actual_model = settings.MODEL_MAPPING.get(model_name, "DeepSeek-V3")
        
        chat_id = str(uuid.uuid4())
        
        files = {
            "entertext": (None, prompt_text),
            "chat_id": (None, chat_id),
            "model": (None, actual_model),
            "chat_group": (None, ""),
        }

        if image_url:
            logger.info(f"æ£€æµ‹åˆ°å›¾ç‰‡ URLï¼Œæ­£åœ¨ä¸‹è½½: {image_url[:100]}...")
            async with httpx.AsyncClient() as client:
                if image_url.startswith("data:image/"):
                    header, encoded = image_url.split(",", 1)
                    image_bytes = base64.b64decode(encoded)
                    file_extension = header.split("/")[1].split(";")[0]
                else:
                    response = await client.get(image_url)
                    response.raise_for_status()
                    image_bytes = response.content
                    file_extension = image_url.split('.')[-1].split('?')[0] or 'jpg'

            files["upload_images"] = (f"image.{file_extension}", image_bytes, f"image/{file_extension}")
            logger.info(f"å›¾ç‰‡å·²å‡†å¤‡å¥½ä¸Šä¼ ï¼Œå¤§å°: {len(image_bytes)} bytes")

        return files, chat_id

    async def get_models(self) -> JSONResponse:
        model_data = {
            "object": "list",
            "data": [
                {"id": name, "object": "model", "created": int(time.time()), "owned_by": "lzA6"}
                for name in settings.MODEL_MAPPING.keys()
            ]
        }
        return JSONResponse(content=model_data)


--- æ–‡ä»¶è·¯å¾„: app\utils\sse_utils.py ---

import json
import time
from typing import Dict, Any, Optional

def create_sse_data(data: Dict[str, Any]) -> str:
    """å°†å­—å…¸æ•°æ®æ ¼å¼åŒ–ä¸º SSE äº‹ä»¶å­—ç¬¦ä¸²ã€‚"""
    return f"data: {json.dumps(data, ensure_ascii=False)}\n\n"

def create_chat_completion_chunk(
    request_id: str,
    model: str,
    content: str,
    finish_reason: Optional[str] = None
) -> Dict[str, Any]:
    """åˆ›å»ºä¸€ä¸ªä¸ OpenAI å…¼å®¹çš„èŠå¤©è¡¥å…¨æµå¼å—ã€‚"""
    return {
        "id": request_id,
        "object": "chat.completion.chunk",
        "created": int(time.time()),
        "model": model,
        "choices": [
            {
                "index": 0,
                "delta": {"content": content},
                "finish_reason": finish_reason
            }
        ]
    }

# --- [æ–°å¢] åˆ›å»ºä¸ OpenAI å…¼å®¹çš„éæµå¼èŠå¤©è¡¥å…¨å“åº” ---
def create_chat_completion_response(
    request_id: str,
    model: str,
    content: str,
    finish_reason: str
) -> Dict[str, Any]:
    """åˆ›å»ºä¸€ä¸ªä¸ OpenAI å…¼å®¹çš„å®Œæ•´ã€éæµå¼èŠå¤©è¡¥å…¨å“åº”ã€‚"""
    return {
        "id": request_id,
        "object": "chat.completion",
        "created": int(time.time()),
        "model": model,
        "choices": [
            {
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": content
                },
                "finish_reason": finish_reason
            }
        ],
        "usage": {
            "prompt_tokens": 0, # æ³¨æ„ï¼šç›®å‰æ— æ³•ä»ä¸Šæ¸¸è·å–å‡†ç¡®çš„ token æ•°
            "completion_tokens": 0,
            "total_tokens": 0
        }
    }

DONE_CHUNK = create_sse_data({"id": "done", "object": "done", "choices": [], "created": int(time.time()), "model": "done"})



